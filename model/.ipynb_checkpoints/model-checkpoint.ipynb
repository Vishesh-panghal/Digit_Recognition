{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd59726-2a32-4206-a8b5-dc055c11108e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329cd7a-104c-4eeb-8992-69b101f63dcc",
   "metadata": {},
   "source": [
    "**Name**: <font color=\"lime\">**Panghal vishesh**</font>  \n",
    "**Specialization**: Machine Learning and Computer Vision  \n",
    "**Date**: Dec/29/2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4fa64-522e-4545-8762-4fc3e0b83de5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d149b-2731-4796-8899-c9a44a6f032e",
   "metadata": {},
   "source": [
    "- # ðŸ¤– ANN vs ðŸ§  CNN: A Comprehensive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a170d-0ae7-48d8-adeb-68a00fd4118f",
   "metadata": {},
   "source": [
    "***\n",
    "<div style=\"background-color: transparent; padding: 10px;\">\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px; text-align: center;\">\n",
    "      <img src=\"https://media.licdn.com/dms/image/v2/D4D12AQGBG0RgeyMHVw/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1692187763032?e=2147483647&v=beta&t=f139Om73qZxbbvCtA-EKhPWUC-YU3qCVRmNCqDM2d3g\" alt=\"ANN\" style=\"width: 600px; height: auto;background-color:white;\">\n",
    "    </td>\n",
    "    <td style=\"padding: 10px; text-align: center;\">\n",
    "      <img src=\"https://media.licdn.com/dms/image/D5612AQGOui8XZUZJSA/article-cover_image-shrink_720_1280/0/1680532048475?e=2147483647&v=beta&t=8aodfukDSrrnnxOVSNobKYJtbtSDB7yC83LUky-Ob68\" alt=\"CNN\" style=\"width: 800px; height: auto;\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53b87b-f064-43b6-b341-17b92064ca3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <font color=gold> ANN<font/>  \n",
    "- An Artificial Neural Network in the field of Artificial intelligence where it attempts to mimic the network of neurons makes up a human brain so that computers will have an option to understand things and make decisions in a human-like manner. The artificial neural network is designed by programming computers to behave simply like interconnected brain cells.\n",
    "\n",
    "## <font color='white'> Key Components of an Artificial Neural Network (ANN) <font/>  \n",
    "1. **Input Layer**:\n",
    "   - The input layer takes in the data features (e.g., numbers, vectors) and passes them to the next layer. Each neuron represents one feature of the input data.\n",
    "2. **Hidden Layers**:\n",
    "   - These layers consist of neurons that process the input data. Neurons in each layer are connected to every neuron in the previous and next layers (fully connected). The hidden layers perform computations and learn the relationships between input features through activation functions.\n",
    "3. **Output Layer**:\n",
    "   - The output layer produces the final result or prediction (e.g., class labels in classification tasks or a continuous value in regression). The number of neurons in this layer corresponds to the number of output classes or target values.\n",
    "4. **Weights and Biases**:\n",
    "   - Weights determine the strength of the connection between neurons, and biases allow the model to shift the activation function to better fit the data. Both are learned during the training process.\n",
    "5. **Activation Functions**:\n",
    "   - Activation functions introduce non-linearity into the network, enabling it to learn complex patterns. Common functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "6. **Loss Function**:\n",
    "   - The loss function measures how far the model's predictions are from the actual target values. During training, the network tries to minimize this loss to improve its performance. Examples include Mean Squared Error (MSE) and Cross-Entropy Loss.\n",
    "7. **Optimizer**:\n",
    "   - Optimizers update the weights and biases during training to minimize the loss function. Examples include Stochastic Gradient Descent (SGD), Adam, and RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1286a76-b587-494c-bd2d-a412ae66f3af",
   "metadata": {},
   "source": [
    "## <font color=gold> CNN<font/>  \n",
    "- An Convolutional Neural Network is a type of deep learning algorithm that is particularly well-suited for image recognition and processing tasks. It is made up of multiple layers, including convolutional layers, pooling layers, and fully connected layers.\n",
    "\n",
    "## <font color='white'> Key Components of an Convolutional Neural Network (CNN) <font/> \n",
    "1. **Convolutional Layers**:\n",
    "   - The core component of a CNN, where filters (kernels) slide over the input image (or feature maps) to detect local patterns like edges, textures, and shapes. The convolution operation applies a filter to the input to produce a feature map.\n",
    "2. **ReLU (Rectified Linear Unit)**:\n",
    "   - After convolution, the ReLU activation function is typically applied to introduce non-linearity. It replaces all negative values in the feature map with zero, allowing the network to learn complex patterns.\n",
    "3. **Pooling Layers**:\n",
    "   - Pooling reduces the spatial dimensions (height and width) of the feature maps, making the network more computationally efficient and helping to avoid overfitting. The most common type is Max Pooling, which selects the maximum value from a small region of the feature map.\n",
    "4. **Fully Connected Layer**:\n",
    "   - After several convolutional and pooling layers, the high-level features are passed to one or more fully connected layers (like those in a regular ANN). These layers are used to make final predictions, such as class labels or regression outputs.\n",
    "5. **Dropout Layer**:\n",
    "   - A regularization technique where random neurons are \"dropped\" during training to prevent overfitting. This forces the network to generalize better.\n",
    "6. **Batch Normalization**:\n",
    "   - A technique to normalize the activations of each layer in the network to improve training speed and stability.\n",
    "7. **Softmax / Sigmoid (Output Layer)**:\n",
    "   - For classification tasks, the output layer often uses a Softmax activation for multi-class classification (converting raw scores into probabilities).\n",
    "   - For binary classification, Sigmoid is often used.\n",
    "8. **Loss Function and Optimizer**:\n",
    "   - Similar to ANN, CNNs use loss functions like Cross-Entropy Loss for classification tasks and an optimizer like Adam to minimize the loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bda77-7716-441c-98d5-9586d6705501",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%; border: 1px solid black; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 10px; text-align: center;\">Aspect</th>\n",
    "    <th style=\"padding: 10px; text-align: center;\">ANN</th>\n",
    "    <th style=\"padding: 10px; text-align: center;\">CNN</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Architecture</td>\n",
    "    <td style=\"padding: 10px;\">Fully connected layers</td>\n",
    "    <td style=\"padding: 10px;\">Convolutional, pooling layers</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Input</td>\n",
    "    <td style=\"padding: 10px;\">1D data (tabular, vector)</td>\n",
    "    <td style=\"padding: 10px;\">2D/3D data (images, videos)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Feature Extraction</td>\n",
    "    <td style=\"padding: 10px;\">Manual</td>\n",
    "    <td style=\"padding: 10px;\">Automatic via convolutions</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Parameter Sharing</td>\n",
    "    <td style=\"padding: 10px;\">No</td>\n",
    "    <td style=\"padding: 10px;\">Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Spatial Hierarchy</td>\n",
    "    <td style=\"padding: 10px;\">No</td>\n",
    "    <td style=\"padding: 10px;\">Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Use Cases</td>\n",
    "    <td style=\"padding: 10px;\">General tasks</td>\n",
    "    <td style=\"padding: 10px;\">Image, video, spatial data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 10px;\">Efficiency</td>\n",
    "    <td style=\"padding: 10px;\">Lower for high-dimensional</td>\n",
    "    <td style=\"padding: 10px;\">Higher for high-dimensional</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42c58e-cc2a-40bf-94e5-9e836708181d",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">Handwritten Digit Recognition (ANN)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46553e0-93b3-4bf2-99b6-bd4e085358c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0951c786-5946-4aee-8e46-8a09a2bf6a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1d9f5-41c6-47e7-bb7e-b71dc5923a7d",
   "metadata": {},
   "source": [
    "## <font color='gold'> Prepare Dataset<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98452816-19cd-4723-a796-4db810bb0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((28, 28)),                 # Resize to 28x28\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))         # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315da368-86f1-4c42-a03a-37b7b6043948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 31715\n",
      "Class to Index Mapping: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageFolder(\"../data/dataset/\",transform=transform)\n",
    "\n",
    "print(f\"Number of samples: {len(dataset)}\")\n",
    "print(f\"Class to Index Mapping: {dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9c3c16-49da-48cf-b642-c306e04e6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdbd007-bd39-4f36-b6f3-ca5e603b4d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 25372\n",
      "Testing Samples: 6343\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8*len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset,[train_size,test_size])\n",
    "\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Testing Samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "923cdc14-345c-4884-b747-5321ffdb3be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Loader\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64,shuffle=False)\n",
    "\n",
    "classes = dataset.classes\n",
    "for images,labels in train_loader:\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title(f\"Label: {classes[labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "    break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7be416-735a-478a-b52d-ba36691e5f8e",
   "metadata": {},
   "source": [
    "## <font color='gold'> Define Model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0f63df-c5b0-4690-8b50-934cc3935ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(num_features, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.network(features)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f40219-9d9b-4afe-a876-4b83d9de48e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label = train_dataset[0]\n",
    "height,width = image.shape[1],image.shape[2]\n",
    "num_features = height*width\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55fcd1f-c81e-4ad2-8844-1e440642ebcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNModel(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN_model = ANNModel(num_features)\n",
    "ANN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76f4a1-3e28-4417-bcfa-74de6f9bc17f",
   "metadata": {},
   "source": [
    "## <font color='gold'> Train Model <font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e735f1-7f31-49b2-883c-998da11dd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77b8c16c-008c-4a20-883e-b87378bd76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ANN_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a26f0521-10ac-41ab-a186-7278cc7f75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8996, Accuracy: 70.71%\n",
      "Epoch [2/50], Loss: 0.4809, Accuracy: 84.60%\n",
      "Epoch [3/50], Loss: 0.3711, Accuracy: 87.77%\n",
      "Epoch [4/50], Loss: 0.2887, Accuracy: 90.49%\n",
      "Epoch [5/50], Loss: 0.2478, Accuracy: 91.84%\n",
      "Epoch [6/50], Loss: 0.2190, Accuracy: 92.55%\n",
      "Epoch [7/50], Loss: 0.1980, Accuracy: 93.30%\n",
      "Epoch [8/50], Loss: 0.1896, Accuracy: 93.55%\n",
      "Epoch [9/50], Loss: 0.1731, Accuracy: 94.13%\n",
      "Epoch [10/50], Loss: 0.1569, Accuracy: 94.78%\n",
      "Epoch [11/50], Loss: 0.1535, Accuracy: 94.80%\n",
      "Epoch [12/50], Loss: 0.1439, Accuracy: 95.14%\n",
      "Epoch [13/50], Loss: 0.1322, Accuracy: 95.63%\n",
      "Epoch [14/50], Loss: 0.1266, Accuracy: 95.66%\n",
      "Epoch [15/50], Loss: 0.1224, Accuracy: 95.75%\n",
      "Epoch [16/50], Loss: 0.1192, Accuracy: 96.05%\n",
      "Epoch [17/50], Loss: 0.1123, Accuracy: 96.01%\n",
      "Epoch [18/50], Loss: 0.1008, Accuracy: 96.50%\n",
      "Epoch [19/50], Loss: 0.0988, Accuracy: 96.55%\n",
      "Epoch [20/50], Loss: 0.0995, Accuracy: 96.59%\n",
      "Epoch [21/50], Loss: 0.0962, Accuracy: 96.75%\n",
      "Epoch [22/50], Loss: 0.0949, Accuracy: 96.80%\n",
      "Epoch [23/50], Loss: 0.0944, Accuracy: 96.75%\n",
      "Epoch [24/50], Loss: 0.0887, Accuracy: 96.96%\n",
      "Epoch [25/50], Loss: 0.0875, Accuracy: 96.99%\n",
      "Epoch [26/50], Loss: 0.0882, Accuracy: 96.94%\n",
      "Epoch [27/50], Loss: 0.0820, Accuracy: 97.10%\n",
      "Epoch [28/50], Loss: 0.0794, Accuracy: 97.28%\n",
      "Epoch [29/50], Loss: 0.0847, Accuracy: 97.23%\n",
      "Epoch [30/50], Loss: 0.0724, Accuracy: 97.45%\n",
      "Epoch [31/50], Loss: 0.0769, Accuracy: 97.41%\n",
      "Epoch [32/50], Loss: 0.0748, Accuracy: 97.38%\n",
      "Epoch [33/50], Loss: 0.0672, Accuracy: 97.67%\n",
      "Epoch [34/50], Loss: 0.0748, Accuracy: 97.43%\n",
      "Epoch [35/50], Loss: 0.0698, Accuracy: 97.56%\n",
      "Epoch [36/50], Loss: 0.0676, Accuracy: 97.71%\n",
      "Epoch [37/50], Loss: 0.0720, Accuracy: 97.45%\n",
      "Epoch [38/50], Loss: 0.0641, Accuracy: 97.82%\n",
      "Epoch [39/50], Loss: 0.0646, Accuracy: 97.88%\n",
      "Epoch [40/50], Loss: 0.0604, Accuracy: 97.88%\n",
      "Epoch [41/50], Loss: 0.0629, Accuracy: 97.75%\n",
      "Epoch [42/50], Loss: 0.0652, Accuracy: 97.88%\n",
      "Epoch [43/50], Loss: 0.0541, Accuracy: 98.13%\n",
      "Epoch [44/50], Loss: 0.0579, Accuracy: 98.05%\n",
      "Epoch [45/50], Loss: 0.0595, Accuracy: 97.96%\n",
      "Epoch [46/50], Loss: 0.0567, Accuracy: 98.02%\n",
      "Epoch [47/50], Loss: 0.0574, Accuracy: 98.05%\n",
      "Epoch [48/50], Loss: 0.0565, Accuracy: 98.07%\n",
      "Epoch [49/50], Loss: 0.0568, Accuracy: 98.05%\n",
      "Epoch [50/50], Loss: 0.0585, Accuracy: 98.02%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    ANN_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten images (MNIST images are 28x28)\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = ANN_model(images)\n",
    "\n",
    "        loss = loss_function(output,labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss +=loss.item()\n",
    "        _,predicted = torch.max(output,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d47b8ba1-35a0-4d3a-be95-923c929833fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.71%\n"
     ]
    }
   ],
   "source": [
    "ANN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = ANN_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fff0d0d-8071-469b-836c-8ef61cce9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35de425c-0e7d-4fdd-9bfe-098994685d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]         100,480\n",
      "       BatchNorm1d-2                  [-1, 128]             256\n",
      "              ReLU-3                  [-1, 128]               0\n",
      "            Linear-4                   [-1, 64]           8,256\n",
      "       BatchNorm1d-5                   [-1, 64]             128\n",
      "              ReLU-6                   [-1, 64]               0\n",
      "            Linear-7                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 109,770\n",
      "Trainable params: 109,770\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model=ANN_model, input_size=(28*28,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de96b97-f4dc-43ff-b183-061b5b665a2d",
   "metadata": {},
   "source": [
    "## <font color='gold'> Export model<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf57173-6fba-474b-b8b1-c17ef8562945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(ANN_model.state_dict(), '../model/(ANN)digit_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a85bd-03b6-465a-8e39-14aea8244407",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460348b-06b6-4ea2-957e-c44c838b18a0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634731a-7c0b-488b-a636-a16bce49071f",
   "metadata": {},
   "source": [
    "# <font color=\"cyan\">Handwritten Digit Recognition (CNN)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9464fcd-82f8-4019-b12e-2383f036fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "# from preprocessing import preprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "267729fc-abd5-498c-8039-2d0a3625c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "Training Samples: 25372\n",
      "Testing Samples: 6343\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='../data/dataset/',transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset)) \n",
    "test_size = len(train_dataset) - train_size \n",
    "\n",
    "train_subset, test_subset = random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "# Train loader\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Test loader\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break\n",
    "\n",
    "print(f\"Training Samples: {len(train_subset)}\")\n",
    "print(f\"Testing Samples: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0371c54f-a925-442f-9e6d-c91adb00e3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    plt.imshow(images[0].squeeze(), cmap='grey')\n",
    "    plt.title(f\"Label: {labels[0]}\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57c470b6-f601-415f-a409-20cafbd2e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f042a3fb-774a-4935-8e12-3a51f34f5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = None  # Will initialize dynamically\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        feature = F.relu(self.conv1(feature))\n",
    "        feature = self.pool(feature)\n",
    "        feature = F.relu(self.conv2(feature))\n",
    "        feature = self.pool(feature)\n",
    "        feature = feature.view(feature.size(0), -1)  # Flatten the feature map\n",
    "        if not self.fc1:  # Initialize fc1 dynamically based on flattened feature size\n",
    "            self.fc1 = nn.Linear(feature.size(1), 128)\n",
    "        feature = F.relu(self.fc1(feature))\n",
    "        feature = self.fc2(feature)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc4c58b-12ca-422b-b156-7df062eccdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model = CNNModel()\n",
    "CNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6fbbb67-1e97-4dd8-a0bd-1a83b76918a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ca71d65-ae48-4b1b-a5d5-9670fc93352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.7727, Accuracy: 75.41%\n",
      "Epoch [2/25], Loss: 0.3755, Accuracy: 88.33%\n",
      "Epoch [3/25], Loss: 0.2929, Accuracy: 91.08%\n",
      "Epoch [4/25], Loss: 0.2563, Accuracy: 92.35%\n",
      "Epoch [5/25], Loss: 0.2185, Accuracy: 93.35%\n",
      "Epoch [6/25], Loss: 0.1995, Accuracy: 93.89%\n",
      "Epoch [7/25], Loss: 0.1753, Accuracy: 94.64%\n",
      "Epoch [8/25], Loss: 0.1689, Accuracy: 94.61%\n",
      "Epoch [9/25], Loss: 0.1572, Accuracy: 95.05%\n",
      "Epoch [10/25], Loss: 0.1555, Accuracy: 95.23%\n",
      "Epoch [11/25], Loss: 0.1404, Accuracy: 95.40%\n",
      "Epoch [12/25], Loss: 0.1276, Accuracy: 96.01%\n",
      "Epoch [13/25], Loss: 0.1217, Accuracy: 96.22%\n",
      "Epoch [14/25], Loss: 0.1183, Accuracy: 96.25%\n",
      "Epoch [15/25], Loss: 0.1244, Accuracy: 96.18%\n",
      "Epoch [16/25], Loss: 0.1168, Accuracy: 96.35%\n",
      "Epoch [17/25], Loss: 0.1052, Accuracy: 96.63%\n",
      "Epoch [18/25], Loss: 0.1153, Accuracy: 96.37%\n",
      "Epoch [19/25], Loss: 0.0947, Accuracy: 96.85%\n",
      "Epoch [20/25], Loss: 0.0913, Accuracy: 97.14%\n",
      "Epoch [21/25], Loss: 0.0938, Accuracy: 97.00%\n",
      "Epoch [22/25], Loss: 0.0989, Accuracy: 96.91%\n",
      "Epoch [23/25], Loss: 0.0840, Accuracy: 97.29%\n",
      "Epoch [24/25], Loss: 0.0898, Accuracy: 97.26%\n",
      "Epoch [25/25], Loss: 0.0818, Accuracy: 97.24%\n"
     ]
    }
   ],
   "source": [
    "CNN_model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    CNN_model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = CNN_model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2a5c7aa-c47e-46c3-a627-b6182faa9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.35%\n"
     ]
    }
   ],
   "source": [
    "CNN_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = CNN_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53d3600-bb70-4983-8c19-ddff2227efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5e1ce8-e0d3-4432-976c-3108f1736fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "image_path = \"../../digit3.png\"\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "# Preprocess the image (match model input requirements)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure single channel\n",
    "    transforms.Resize((28, 28)),                 # Resize to 28x28\n",
    "    transforms.ToTensor(),                       # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))         # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755efb6-4801-4527-8860-906596a04567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c3bf3-99f5-4b16-a127-28f5d8c1b532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929499b3-cfe6-4f3b-ab33-6c36efaa6cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b38c1ce-c7bf-4338-801e-1508b7afeebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
      "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
      "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
      "            Linear-5                  [-1, 128]         401,536\n",
      "            Linear-6                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 421,642\n",
      "Trainable params: 421,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 1.61\n",
      "Estimated Total Size (MB): 1.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model=CNN_model, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92441727-368b-447a-a8c1-35c90ba41fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNN_model.state_dict(), '../model/(CNN)digit_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fded686-d85a-4ae9-b452-31be0a461cae",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab77edf-2aa8-4f76-99e3-96648590fbe3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a80c1-676f-486f-8b66-7a4517ac4e30",
   "metadata": {},
   "source": [
    "## <font color=gold> Observation: <font/>\n",
    "1. **Model Performance and Insights**:\n",
    "   - The **ANN model** achieved a training accuracy of **98.02%** and a test accuracy of **91.71**\n",
    "   - The **CNN model** achieved a training accuracy of **97.24%** and a test accuracy of **92.35%**, which highlights its ability to generalize well to unseen data from the same distribution.\n",
    "\n",
    "2. **Performance on Scanned vs. Real-World Images**:\n",
    "   - ANN:\n",
    "     - The model performed exceptionally well on **scanned images**, with predictions closely matching ground truth,but not on Real-World Images.\n",
    "    - CNN:\n",
    "      - The model performed exceptionally well on **scanned images**,as well as on **real-world images** with predictions closely matching ground truth.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a5f85-76e2-41a5-82f0-8352fad5d418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Handwritten_Digit_Recognition)",
   "language": "python",
   "name": "handwritten_digit_recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
